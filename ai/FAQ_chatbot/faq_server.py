import os

from flask import Flask, request, jsonify ## í”Œë¼ìŠ¤í¬

# ìµœì‹  LangChain ëª¨ë“ˆ  (v0.2 ì´ìƒ ê¸°ì¤€)
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import Chroma
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate


# OpenAI API í‚¤ ì„¤ì •
## í›„ì— azure í˜•ì‹ìœ¼ë¡œ ì¶”ê°€

# íŒŒì¼ ê²½ë¡œ
file_path = os.path.join(os.path.dirname(__file__), "faq_service.txt")
persist_path = os.path.join(os.path.dirname(__file__), "db_service_")

# í…ìŠ¤íŠ¸ ë¶„í•  ë° ë²¡í„° DB ìƒì„±
def create_vector_store(file_path, persist_directory):  ## í…ìŠ¤íŠ¸ íŒŒì¼ ë¶ˆëŸ¬ì™€ì„œ ë¶„í• 
    loader = TextLoader(file_path, encoding="utf-8")
    documents = loader.load()
    
    # ë¬¸ì„œ ë¶„í• 
    text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=20)
    docs = text_splitter.split_documents(documents)

    # ë²¡í„° db ìƒì„±
    embeddings = OpenAIEmbeddings()
    vectordb = Chroma.from_documents(documents=docs,
                                     embedding=embeddings, 
                                     persist_directory=persist_directory
                                    )
    return vectordb

# ì±—ë´‡ ì²´ì¸ ìƒì„±
def get_chatbot(vectorstore):
    llm = ChatOpenAI(temperature=0.4)
    memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

    # AI í”„ë¡¬í”„íŠ¸ ì‘ì„±
    qa_prompt = PromptTemplate.from_template(
      """ë‹¹ì‹ ì€ ì¹œì ˆí•œ ê³ ê°ì„¼í„° ìƒë‹´ì›ì…ë‹ˆë‹¤.
      ì•„ë˜ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ê³ ê°ì˜ ì§ˆë¬¸ì— ìì—°ìŠ¤ëŸ½ê³  ê³µì†í•˜ê²Œ ì‘ë‹µí•˜ì„¸ìš”.
      ë¬¸ì„œì— ê´€ë ¨ ë‚´ìš©ì´ ì—†ì„ ê²½ìš°, ì£„ì†¡í•˜ë‹¤ëŠ” ë§ê³¼ í•¨ê»˜ ì•ˆë‚´ ë¶ˆê°€ ì‚¬ìœ ë¥¼ ë¶€ë“œëŸ½ê²Œ ì„¤ëª…í•˜ì„¸ìš”.

      ë¬¸ì„œ:
      {context}

      ê³ ê° ì§ˆë¬¸:
      {question}

      ë‹µë³€:"""
    )  ## í”„ë¡¬í”„íŠ¸ ì„¤ì •ì„ í†µí•´ FAQ DBì— ì—†ëŠ” ì§ˆë¬¸ì´ ë“¤ì–´ì˜¬ ê²½ìš° ì •ë³´ ì œê³µì´ ë¶ˆê°€ëŠ¥í•˜ë‹¤ê³  ë‹µë³€

    qa_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=vectorstore.as_retriever(search_kwargs={"k": 2}),
        memory=memory,
        combine_docs_chain_kwargs={"prompt": qa_prompt},
        verbose=False
    )
    return qa_chain

############################# ì±—ë´‡ ì‹œì‘ ################################
# ğŸ”§ Flask ì•± ì´ˆê¸°í™”
app = Flask(__name__)

# vector db í˜¸ì¶œ :
vectordb = create_vector_store(file_path, persist_path)

# ì±—ë´‡ ì‹¤í–‰ ì‹œ ë‚˜ì˜¤ëŠ” ë¬¸êµ¬
print("ì•ˆë…•í•˜ì„¸ìš”! ì•½ì‚¬ë´‡ì…ë‹ˆë‹¤. - faq_server.py:76")
print("[ë³¸ì‚¬ ì •ì±… ê´€ë ¨] [ìš´ì˜ í”„ë¡œì„¸ìŠ¤ ê´€ë ¨] [ë°˜ë³µ ìƒë‹´/ê³ ê° ì‘ëŒ€ ê´€ë ¨] [ë³µì•½ì§€ë„ ê´€ë ¨] [ë°˜í’ˆ/êµí™˜/í´ë ˆì„ ê´€ë ¨]  import os.py:76 - faq_server.py:77")
print("ìœ„ì˜ ì¹´í…Œê³ ë¦¬ ì•ˆì—ì„œì˜ ì§ˆë¬¸ì— ëŒ€ë‹µí•´ë“œë¦½ë‹ˆë‹¤ \n - faq_server.py:78")
print("## exitë¥¼ ì…ë ¥ ì‹œ ì±„íŒ…ì´ ì¢…ë£Œë©ë‹ˆë‹¤\n - faq_server.py:79")

# ì²´ì¸ ë¡œë“œ
chatbot = get_chatbot(vectordb)

# ğŸ“© ì§ˆë¬¸ ë°›ëŠ” ì—”ë“œí¬ì¸íŠ¸
@app.route('/chat', methods=['POST'])
def chat():
    data = request.get_json()
    question = data.get('question', '')

    if not question:
        return jsonify({'error': 'ì§ˆë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.'}), 400

    try:
        result = chatbot.invoke(question)
        return jsonify({'answer': result['answer']})
    except Exception as e:
        return jsonify({'error': str(e)}), 500

# ğŸ Flask ì‹¤í–‰
if __name__ == '__main__':
    print("ğŸš€ ì•½ì‚¬ë´‡ Flask ì„œë²„ ì‹¤í–‰ ì¤‘... (http://localhost:5000/chat)  import os.py:100 - faq_server.py:101")
    app.run(debug=True)